{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DVM\n",
    "Pre-proceed DVM data and create train/val/test datasets\n",
    "* Tabular data (.csv)\n",
    "* Image paths (.pt)\n",
    "* Labels (.pt)\n",
    "* Tabular lengths (.pt): record the number of unique values for each column\n",
    "\n",
    "Based on MMCL code https://github.com/paulhager/MMCL-Tabular-Imaging/blob/main/data/create_dvm_dataset.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from os.path import join\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "pd.options.display.max_columns = 700\n",
    "\n",
    "# TODO: change this to the path of your DVM data directory\n",
    "BASE = '/mnt/data/kgutjahr/datasets/DVM'\n",
    "TABLES = join(BASE, 'tables')\n",
    "FEATURES = join(BASE, 'images') \n",
    "\n",
    "front_view_only = False\n",
    "\n",
    "ANALYSIS = join(BASE, 'analysis')\n",
    "\n",
    "def conf_matrix_from_matrices(mat_gt, mat_pred):\n",
    "  overlap_and = (mat_pred & mat_gt)\n",
    "  tp = overlap_and.sum()\n",
    "  fp = mat_pred.sum()-overlap_and.sum()\n",
    "  fn = mat_gt.sum()-overlap_and.sum()\n",
    "  tn = mat_gt.shape[0]**2-(tp+fp+fn)\n",
    "  return tp, fp, fn, tn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_or_save(obj, path, index=None, header=None):\n",
    "  if isinstance(obj, pd.DataFrame):\n",
    "    if index is None or header is None:\n",
    "      raise ValueError('Index and header must be specified for saving a dataframe')\n",
    "    if os.path.exists(path):\n",
    "      if not header:\n",
    "        saved_df = pd.read_csv(path,header=None)\n",
    "      else:\n",
    "        saved_df = pd.read_csv(path)\n",
    "      naked_df = saved_df.reset_index(drop=True)\n",
    "      naked_df.columns = range(naked_df.shape[1])\n",
    "      naked_obj = obj.reset_index(drop=not index)\n",
    "      naked_obj.columns = range(naked_obj.shape[1])\n",
    "      if naked_df.round(6).equals(naked_obj.round(6)):\n",
    "        return\n",
    "      else:\n",
    "        diff = (naked_df.round(6) == naked_obj.round(6))\n",
    "        diff[naked_df.isnull()] = naked_df.isnull() & naked_obj.isnull()\n",
    "        assert diff.all().all(), \"Dataframe is not the same as saved dataframe\"\n",
    "    else:\n",
    "      path_dir = os.path.dirname(path)\n",
    "      if not os.path.exists(path_dir):\n",
    "        os.makedirs(path_dir)\n",
    "      obj.to_csv(path, index=index, header=header)\n",
    "  else:\n",
    "    if os.path.exists(path):\n",
    "      saved_obj = torch.load(path)\n",
    "      if isinstance(obj, list):\n",
    "        for i in range(len(obj)):\n",
    "          check_array_equality(obj[i], saved_obj[i])\n",
    "      else:\n",
    "        check_array_equality(obj, saved_obj)\n",
    "    else:\n",
    "      print(f'Saving to {path}')\n",
    "      torch.save(obj, path)\n",
    "\n",
    "\n",
    "def check_array_equality(ob1, ob2):\n",
    "  if torch.is_tensor(ob1) or isinstance(ob1, np.ndarray):\n",
    "    assert (ob2 == ob1).all()\n",
    "  else:\n",
    "    assert ob2 == ob1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Tabular Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_data = pd.read_csv(join(TABLES, 'Ad_table.csv'))\n",
    "ad_data.rename(columns={' Genmodel': 'Genmodel', ' Genmodel_ID': 'Genmodel_ID'}, inplace=True)\n",
    "\n",
    "basic_data = pd.read_csv(join(TABLES, 'Basic_table.csv'))\n",
    "\n",
    "image_data = pd.read_csv(join(TABLES, 'Image_table.csv'))\n",
    "image_data.rename(columns={' Image_ID': 'Image_ID', ' Image_name': 'Image_name', ' Predicted_viewpoint':'Predicted_viewpoint', ' Quality_check':'Quality_check'}, inplace=True)\n",
    "\n",
    "price_data = pd.read_csv(join(TABLES, 'Price_table.csv'))\n",
    "price_data.rename(columns={' Genmodel': 'Genmodel', ' Genmodel_ID': 'Genmodel_ID', ' Year': 'Year', ' Entry_price': 'Entry_price'}, inplace=True)\n",
    "\n",
    "sales_data = pd.read_csv(join(TABLES, 'Sales_table.csv'))\n",
    "sales_data.rename(columns={'Genmodel ': 'Genmodel', 'Genmodel_ID ': 'Genmodel_ID'}, inplace=True)\n",
    "\n",
    "trim_data = pd.read_csv(join(TABLES, 'Trim_table.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser_adv_id(x):\n",
    "  split = x[\"Image_ID\"].split('$$')\n",
    "  return f\"{split[0]}$${split[1]}\"\n",
    "\n",
    "image_data[\"Adv_ID\"] = image_data.apply(lambda x: parser_adv_id(x), axis=1)\n",
    "if front_view_only:\n",
    "  image_data = image_data[(image_data[\"Quality_check\"]==\"P\")&(image_data[\"Predicted_viewpoint\"]==0)]\n",
    "image_data.drop_duplicates(subset=['Adv_ID'], inplace=True)\n",
    "image_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ad_data))\n",
    "feature_df = ad_data.merge(price_data[['Genmodel_ID', 'Entry_price', 'Year']], left_on=['Genmodel_ID','Reg_year'], right_on=['Genmodel_ID','Year'])\n",
    "print(len(feature_df))\n",
    "feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = feature_df.merge(image_data[['Adv_ID', 'Image_name', 'Predicted_viewpoint']], left_on=['Adv_ID'], right_on=['Adv_ID'])\n",
    "assert data_df[\"Adv_ID\"].is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_engine_size(x):\n",
    "  return float(x['Engin_size'][:-1])\n",
    "\n",
    "data_df.dropna(inplace=True)\n",
    "data_df['Engine_size'] = data_df.apply(lambda x: extract_engine_size(x), axis=1)\n",
    "data_df.drop(columns=['Engin_size'], inplace=True)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['Full_Model'] = data_df['Maker'] + ' ' + data_df['Genmodel']\n",
    "id_to_model_mapping = data_df.drop_duplicates(subset='Genmodel_ID').set_index('Genmodel_ID')['Full_Model'].to_dict()\n",
    "id_to_model_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_df = data_df.loc[:,'Adv_ID']\n",
    "image_name_df = data_df.loc[:,'Image_name']\n",
    "viewpoint_df = data_df.loc[:,'Predicted_viewpoint']\n",
    "\n",
    "continuous_df = data_df.loc[:,(\n",
    "  'Adv_year',\n",
    "  'Adv_month',\n",
    "  'Reg_year',\n",
    "  'Runned_Miles',\n",
    "  'Price',\n",
    "  'Seat_num',\n",
    "  'Door_num',\n",
    "  'Entry_price', \n",
    "  'Engine_size'\n",
    "  )]\n",
    "\n",
    "categorical_ids = ['Color',\n",
    "  'Bodytype',\n",
    "  'Gearbox',\n",
    "  'Fuel_type',\n",
    "  'Genmodel_ID']\n",
    "\n",
    "\n",
    "\n",
    "categorical_df = data_df.loc[:,categorical_ids]\n",
    "\n",
    "category_mappings = {}\n",
    "# Convert to category and store mappings BEFORE applying .cat.codes\n",
    "\n",
    "continuous_df['Runned_Miles'] = pd.to_numeric(continuous_df['Runned_Miles'], errors='coerce')\n",
    "continuous_df['Price'] = pd.to_numeric(continuous_df['Price'], errors='coerce')\n",
    "#\n",
    "## normalize\n",
    "continuous_df=(continuous_df-continuous_df.mean())/continuous_df.std()\n",
    "\n",
    "categorical_df['Color'] = categorical_df['Color'].astype('category')\n",
    "categorical_df['Genmodel_ID'] = categorical_df['Genmodel_ID'].astype('category')\n",
    "\n",
    "category_mappings['Color'] = dict(enumerate(categorical_df['Color'].cat.categories))\n",
    "category_mappings['Genmodel_ID'] = dict(enumerate(categorical_df['Genmodel_ID'].cat.categories))\n",
    "\n",
    "categorical_df['Color'] = categorical_df['Color'].cat.codes\n",
    "categorical_df['Genmodel_ID'] = categorical_df['Genmodel_ID'].cat.codes\n",
    "print(category_mappings['Color'])\n",
    "print(category_mappings['Genmodel_ID'])\n",
    "#\n",
    "categorical_df['Bodytype'] = categorical_df['Bodytype'].astype('category')\n",
    "categorical_df['Gearbox'] = categorical_df['Gearbox'].astype('category')\n",
    "categorical_df['Fuel_type'] = categorical_df['Fuel_type'].astype('category')\n",
    "categorical_df['Genmodel_ID'] = categorical_df['Genmodel_ID'].astype('category')\n",
    "#\n",
    "cat_columns = categorical_df.select_dtypes(['category']).columns\n",
    "cat_copy_df = categorical_df.copy()\n",
    "#\n",
    "categorical_df[cat_columns] = categorical_df[cat_columns].apply(lambda x: x.cat.codes)\n",
    "#\n",
    "data_df = pd.concat([id_df, continuous_df, categorical_df, image_name_df, viewpoint_df], axis=1)\n",
    "data_df.dropna(inplace=True)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_label_dict = {k: id_to_model_mapping[v] for k, v in category_mappings['Genmodel_ID'].items()}\n",
    "final_label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_population = 100\n",
    "values = (data_df.value_counts(subset=['Genmodel_ID'])>=minimum_population).values\n",
    "codes = (data_df.value_counts(subset=['Genmodel_ID'])>=minimum_population).index\n",
    "populated_codes = []\n",
    "for i, v in enumerate(values):\n",
    "  if v:\n",
    "    populated_codes.append(int(codes[i][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(populated_codes)\n",
    "populated_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data_df = data_df[data_df['Genmodel_ID'].isin(populated_codes)]\n",
    "map = {}\n",
    "for i,l in enumerate(filtered_data_df['Genmodel_ID'].unique()):\n",
    "  map[l] = i\n",
    "filtered_data_df['Genmodel_ID'] = filtered_data_df['Genmodel_ID'].map(map)\n",
    "filtered_data_df.to_csv(join(FEATURES, f'filtered_data_df.csv'))\n",
    "filtered_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_labels_dict = {k: v for k, v in final_label_dict.items() if k in populated_codes}\n",
    "filtered_labels_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Genmodel_ID_counts = filtered_data_df[\"Genmodel_ID\"].value_counts()\n",
    "Genmodel_ID_percentages = filtered_data_df[\"Genmodel_ID\"].value_counts(normalize=True) * 100\n",
    "\n",
    "Genmodel_ID_summary = pd.DataFrame({\n",
    "    'Count': Genmodel_ID_counts,\n",
    "    'Percentage': Genmodel_ID_percentages.round(3)\n",
    "})\n",
    "Genmodel_ID_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_indices = []\n",
    "for indx, row in filtered_data_df.iterrows():\n",
    "    im_name = row['Image_name']\n",
    "    split = im_name.split('$$')\n",
    "    path = join(FEATURES, split[0], split[1], split[2], split[3], im_name)\n",
    "    if not os.path.exists(path):\n",
    "        bad_indices.append(path)\n",
    "        \n",
    "addendum = '_all_views'\n",
    "check_or_save(bad_indices, join(FEATURES, f'bad_indices_train{addendum}.pt'))\n",
    "check_or_save(bad_indices, join(FEATURES, f'bad_indices_val{addendum}.pt'))\n",
    "print(bad_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ids = list(filtered_data_df['Adv_ID'])\n",
    "addendum = '_all_views'\n",
    "non_feature_columns = ['Adv_ID', 'Image_name', 'Predicted_viewpoint', 'Genmodel_ID']\n",
    "if front_view_only:\n",
    "  train_set_ids, test_ids = train_test_split(_ids, test_size=0.1, random_state=2022)\n",
    "  train_ids, val_ids = train_test_split(train_set_ids, test_size=0.2, random_state=2022)\n",
    "  \n",
    "  bad_indices_train = torch.load(join(FEATURES, f'bad_indices_train{addendum}.pt'))\n",
    "  bad_indices_val = torch.load(join(FEATURES, f'bad_indices_val{addendum}.pt'))\n",
    "\n",
    "  print(f'Val length before {len(val_ids)}')\n",
    "  for _id in bad_indices_val:\n",
    "    if _id in val_ids:\n",
    "      val_ids.remove(_id)\n",
    "  print(f'Val length after {len(val_ids)}')\n",
    "\n",
    "  print(f'Train length before {len(train_ids)}')\n",
    "  for _id in bad_indices_train:\n",
    "    if _id in train_ids:\n",
    "      train_ids.remove(_id)\n",
    "  print(f'Train length after {len(train_ids)}')\n",
    "else:\n",
    "  addendum = '_all_views'\n",
    "  train_set_ids, test_ids = train_test_split(_ids, test_size=0.5, random_state=2022, stratify=filtered_data_df['Genmodel_ID'])\n",
    "  train_ids, val_ids = train_test_split(train_set_ids, test_size=0.2, random_state=2022, stratify=filtered_data_df[filtered_data_df['Adv_ID'].isin(train_set_ids)]['Genmodel_ID'])\n",
    "\n",
    "check_or_save(train_ids, join(FEATURES, f'train_ids{addendum}.pt'))\n",
    "check_or_save(val_ids, join(FEATURES, f'val_ids{addendum}.pt'))\n",
    "check_or_save(test_ids, join(FEATURES, f'test_ids{addendum}.pt'))\n",
    "\n",
    "train_df = filtered_data_df.set_index('Adv_ID').loc[train_ids]\n",
    "val_df = filtered_data_df.set_index('Adv_ID').loc[val_ids]\n",
    "test_df = filtered_data_df.set_index('Adv_ID').loc[test_ids]\n",
    "\n",
    "train_labels_all = list(train_df['Genmodel_ID'])\n",
    "val_labels_all = list(val_df['Genmodel_ID'])\n",
    "test_labels_all = list(test_df['Genmodel_ID'])\n",
    "\n",
    "check_or_save(train_labels_all, join(FEATURES,f'labels_model_all_train{addendum}.pt'))\n",
    "check_or_save(val_labels_all, join(FEATURES,f'labels_model_all_val{addendum}.pt'))\n",
    "check_or_save(test_labels_all, join(FEATURES,f'labels_model_all_test{addendum}.pt'))\n",
    "\n",
    "check_or_save(train_df.loc[:,~train_df.columns.isin(non_feature_columns)],join(FEATURES,f'dvm_features_train_noOH{addendum}.csv'), index=False, header=False)\n",
    "check_or_save(val_df.loc[:,~val_df.columns.isin(non_feature_columns)],join(FEATURES,f'dvm_features_val_noOH{addendum}.csv'), index=False, header=False)\n",
    "check_or_save(test_df.loc[:,~test_df.columns.isin(non_feature_columns)],join(FEATURES,f'dvm_features_test_noOH{addendum}.csv'), index=False, header=False)\n",
    "\n",
    "check_or_save(train_df, join(FEATURES,f'dvm_full_features_train_noOH{addendum}.csv'), index=True, header=True)\n",
    "check_or_save(val_df, join(FEATURES,f'dvm_full_features_val_noOH{addendum}.csv'), index=True, header=True)\n",
    "check_or_save(test_df, join(FEATURES,f'dvm_full_features_test_noOH{addendum}.csv'), index=True, header=True)\n",
    "\n",
    "lengths = [1 for i in range(len(continuous_df.columns))]\n",
    "\n",
    "if 'Genmodel_ID' in categorical_ids:\n",
    "  categorical_ids.remove('Genmodel_ID')\n",
    "max_list = list(filtered_data_df[categorical_ids].max(axis=0))\n",
    "max_list = [i+1 for i in max_list]\n",
    "lengths = lengths + max_list\n",
    "check_or_save(lengths, join(FEATURES, f'tabular_lengths{addendum}.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paths(df):\n",
    "  paths = []\n",
    "  for indx, row in df.iterrows():\n",
    "      im_name = row['Image_name']\n",
    "      split = im_name.split('$$')\n",
    "      path = join(FEATURES, split[0], split[1], split[2], split[3], im_name)\n",
    "      paths.append(path)\n",
    "  return paths\n",
    "\n",
    "# For big dataset need to save only paths to load live\n",
    "addendum = '_all_views'\n",
    "train_df = pd.read_csv(join(FEATURES,f'dvm_full_features_train_noOH{addendum}.csv'))\n",
    "val_df = pd.read_csv(join(FEATURES,f'dvm_full_features_val_noOH{addendum}.csv'))\n",
    "test_df = pd.read_csv(join(FEATURES,f'dvm_full_features_test_noOH{addendum}.csv'))\n",
    "\n",
    "for df, name in zip([train_df, val_df, test_df], ['train', 'val', 'test']):\n",
    "  paths = get_paths(df)\n",
    "  check_or_save(paths, join(FEATURES, f'{name}_paths{addendum}.pt'))\n",
    "  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Low Data Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_data_split(df, nclasses, split):\n",
    "  critical_ids = df.groupby('Genmodel_ID', as_index=False).head(n=1)['Adv_ID']\n",
    "  other_ids = df.loc[~df['Adv_ID'].isin(critical_ids)]['Adv_ID'].values\n",
    "  to_fill_size = (int(len(df)*split)-len(critical_ids))\n",
    "  stratify = None\n",
    "  if to_fill_size >= nclasses:\n",
    "    stratify = df.set_index('Adv_ID').loc[other_ids]['Genmodel_ID']\n",
    "  if to_fill_size > 0:\n",
    "    _, low_data_ids = train_test_split(other_ids, test_size=to_fill_size, random_state=2023, stratify=stratify)\n",
    "  else:\n",
    "    low_data_ids = []\n",
    "  new_ids = np.concatenate([critical_ids,low_data_ids])\n",
    "  return new_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addendum = '_all_views'\n",
    "# data_str = 'images'\n",
    "data_str = 'paths'\n",
    "location = \"\"\n",
    "non_feature_columns = ['Image_name', 'Genmodel_ID', 'Predicted_viewpoint', 'Adv_ID']\n",
    "nclasses = 151\n",
    "if addendum=='_all_views':\n",
    "  #data_str = 'paths'\n",
    "  #location = '_server'\n",
    "  nclasses = 286\n",
    "\n",
    "\n",
    "for k, prev_k in zip([0.01],['']):\n",
    "  df = pd.read_csv(join(FEATURES,f'dvm_full_features_train_noOH{addendum}{prev_k}.csv'))\n",
    "  ids = torch.load(join(FEATURES, f'train_ids{addendum}{prev_k}.pt'))\n",
    "  ims = torch.load(join(FEATURES, f'train_{data_str}{addendum}{location}{prev_k}.pt'))\n",
    "  labels = torch.load(join(FEATURES, f'labels_model_all_train{addendum}{prev_k}.pt'))\n",
    "  low_data_ids = low_data_split(df, nclasses, k)\n",
    "  true_false_mask = [i in low_data_ids for i in ids]\n",
    "  ld = [id for id in ids if id in low_data_ids]\n",
    "  low_data_ids = ld\n",
    "  low_data_df = df.loc[true_false_mask]\n",
    "  if addendum=='_all_views' and not data_str=='images':\n",
    "    ims = np.array(ims)\n",
    "  else:  \n",
    "    ims = torch.tensor(ims)\n",
    "  low_data_ims = ims[true_false_mask]\n",
    "  low_data_labels = [labels[i] for i in range(len(ids)) if ids[i] in low_data_ids]\n",
    "\n",
    "\n",
    "  check_or_save(low_data_df.loc[:,~low_data_df.columns.isin(non_feature_columns)], join(FEATURES,f'dvm_features_train_noOH{addendum}_{k}.csv'), index=False, header=False)\n",
    "  check_or_save(low_data_df, join(FEATURES,f'dvm_full_features_train_noOH{addendum}_{k}.csv'), index=False, header=True)\n",
    "  check_or_save(low_data_ims, join(FEATURES, f'train_{data_str}{addendum}{location}_{k}.pt'))\n",
    "  check_or_save(low_data_ids, join(FEATURES, f'train_ids{addendum}_{k}.pt'))\n",
    "  check_or_save(low_data_labels, join(FEATURES, f'labels_model_all_train{addendum}_{k}.pt'))\n",
    "  \n",
    "  # create unlabeled datasets\n",
    "  # Unlabeled: everything not in low_data_ids\n",
    "  unlabeled_ids = [id for id in ids if id not in low_data_ids]\n",
    "  unlabeled_mask = [i in unlabeled_ids for i in ids]\n",
    "  unlabeled_df = df.loc[unlabeled_mask]\n",
    "#\n",
    "  if addendum == '_all_views' and not data_str == 'images':\n",
    "      ims = np.array(ims)\n",
    "  else:\n",
    "      ims = torch.tensor(ims)\n",
    "#\n",
    "  unlabeled_ims = ims[unlabeled_mask]\n",
    "  unlabeled_labels = [labels[i] for i in range(len(ids)) if ids[i] in unlabeled_ids]\n",
    "  #\n",
    "  check_or_save(unlabeled_df.loc[:, ~unlabeled_df.columns.isin(non_feature_columns)],\n",
    "              join(FEATURES, f'dvm_features_train_noOH{addendum}_{1-k}.csv'),\n",
    "              index=False, header=False)\n",
    "\n",
    "  check_or_save(unlabeled_df,\n",
    "                join(FEATURES, f'dvm_full_features_train_noOH{addendum}_{1-k}.csv'),\n",
    "                index=False, header=True)\n",
    "\n",
    "  check_or_save(unlabeled_ims,\n",
    "                join(FEATURES, f'train_{data_str}{addendum}{location}_{1-k}.pt'))\n",
    "\n",
    "  check_or_save(unlabeled_ids,\n",
    "                join(FEATURES, f'train_ids{addendum}_{1-k}.pt'))\n",
    "\n",
    "  check_or_save(unlabeled_labels,\n",
    "                join(FEATURES, f'labels_model_all_train{addendum}_{1-k}.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 'train'\n",
    "for k in [0.1, 0.01]:\n",
    "  low_data_ids = torch.load(join(FEATURES, f'{split}_ids{addendum}_{k}.pt'))\n",
    "  low_data_df = pd.read_csv(join(FEATURES,f'dvm_full_features_{split}_noOH{addendum}_{k}.csv'))\n",
    "  print(low_data_df.value_counts('Genmodel_ID'))\n",
    "  print(len(low_data_ids))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import torch\n",
    "from os.path import join\n",
    "\n",
    "# TODO: change this to the path of your DVM data directory\n",
    "BASE = '/mnt/data/kgutjahr/datasets/DVM'\n",
    "TABLES = join(BASE, 'tables')\n",
    "FEATURES = join(BASE, 'images')\n",
    "\n",
    "train_images_paths = torch.load(join(FEATURES, f'val_paths_all_views.pt'))\n",
    "\n",
    "from PIL import Image\n",
    "train_images = []\n",
    "for tr_img_path in train_images_paths:\n",
    "    train_images.append(Image.open(tr_img_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size=128\n",
    "\n",
    "transform = transforms.Compose([\n",
    "      transforms.ToTensor(),\n",
    "      transforms.RandomApply([transforms.ColorJitter(brightness=0.8, contrast=0.8, saturation=0.8)], p=0.8),\n",
    "      transforms.RandomGrayscale(p=0.2),\n",
    "      transforms.RandomApply([transforms.GaussianBlur(kernel_size=29, sigma=(0.1, 2.0))],p=0.5),\n",
    "      transforms.RandomResizedCrop(size=(img_size,img_size), scale=(0.2, 1.0), ratio=(0.75, 1.3333333333333333)),\n",
    "      transforms.RandomHorizontalFlip(p=0.5),\n",
    "      transforms.Resize(size=(img_size,img_size)),\n",
    "      transforms.Lambda(lambda x : x.float())\n",
    "    ])\n",
    "\n",
    "im = train_images[3]\n",
    "im_t = transform(im)\n",
    "_ = plt.imshow(im_t.permute(1,2,0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Physical Features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding missing values to physical table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill using other values\n",
    "physical_df_orig = pd.read_csv(join(TABLES,'Ad_table (extra).csv'))\n",
    "physical_df_orig.rename(columns={' Genmodel_ID':'Genmodel_ID', ' Genmodel':'Genmodel'}, inplace=True)\n",
    "\n",
    "# Manual touches\n",
    "\n",
    "# Peugeot RCZ\n",
    "physical_df_orig.loc[physical_df_orig['Genmodel_ID'] == '69_36','Wheelbase']=2612\n",
    "# Ford Grand C-Max\n",
    "physical_df_orig.loc[physical_df_orig['Genmodel_ID'] == '29_20','Wheelbase']=2788 \n",
    "\n",
    "def fill_from_other_entry(row):\n",
    "    for attr in ['Wheelbase', 'Length', 'Width', 'Height']:\n",
    "        if pd.isna(row[attr]) or row[attr]==0:\n",
    "            other_rows = physical_df_orig.loc[physical_df_orig['Genmodel_ID']==row['Genmodel_ID']]\n",
    "            other_rows.dropna(subset=[attr], inplace=True)\n",
    "            other_rows.drop_duplicates(subset=[attr], inplace=True)\n",
    "            other_rows = other_rows[other_rows[attr]>0]\n",
    "            if len(other_rows)>0:\n",
    "                row[attr] = other_rows[attr].values[0]\n",
    "    return row\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "physical_df_orig = physical_df_orig.progress_apply(fill_from_other_entry, axis=1)\n",
    "\n",
    "physical_df_orig.to_csv(join(TABLES,'Ad_table_physical_filled.csv'), index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add physical attributes to features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add jitter to physical dimensions so they aren't just labels\n",
    "def add_jitter(x, jitter=50):\n",
    "    return x + random.randint(-jitter, jitter)\n",
    "\n",
    "random.seed(2022)\n",
    "physical_df = pd.read_csv(join(TABLES,'Ad_table_physical_filled.csv'))\n",
    "for attr in ['Wheelbase', 'Length', 'Width', 'Height']:\n",
    "    physical_df[attr] = physical_df[attr].apply(add_jitter)\n",
    "physical_df.to_csv(join(TABLES,'Ad_table_physical_filled_jittered_50.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ford ranger (29_30) has wrong height. Missing 1 in front... 805.0 instead of 1805.0\n",
    "# Mercedes Benz (59_29) wrong wheelbase, 5246.0 instead of 3106\n",
    "# Kia Rio (43_9) wrong wheelbase, 4065.0 instead of 2580\n",
    "# FIXED\n",
    "\n",
    "def create_feature_files(k: str, splits: list[str]):\n",
    "    for v in ['_all_views']:\n",
    "        for split in splits:  #, ,'val', 'test'\n",
    "            features_df = pd.read_csv(join(FEATURES,f'dvm_full_features_{split}_noOH{v}{k}.csv'))\n",
    "            merged_df = features_df.merge(physical_df, on='Adv_ID')\n",
    "            physical_only_df = merged_df[['Wheelbase','Height','Width','Length','Bodytype']]\n",
    "\n",
    "            for attr in ['Wheelbase','Height','Width','Length']:\n",
    "                assert merged_df[attr].isna().sum()==0\n",
    "                assert (merged_df[attr]==0).sum()==0\n",
    "\n",
    "            # normalize physical attributes\n",
    "            for attr in ['Wheelbase','Height','Width','Length']:\n",
    "                merged_df[attr] = (merged_df[attr]-merged_df[attr].mean())/merged_df[attr].std()\n",
    "                physical_only_df[attr] = (physical_only_df[attr]-physical_only_df[attr].mean())/physical_only_df[attr].std()\n",
    "\n",
    "            # Drop unwanted cols\n",
    "            non_feature_columns = ['Adv_ID', 'Image_name', 'Genmodel_ID']\n",
    "            if v == '_all_views':\n",
    "                non_feature_columns.append('Predicted_viewpoint')\n",
    "            merged_df = merged_df.drop(non_feature_columns, axis=1)\n",
    "\n",
    "            merged_df_cols = merged_df.columns.tolist()\n",
    "            rearranged_cols = merged_df_cols[-4:]+merged_df_cols[:-4]\n",
    "            merged_df = merged_df[rearranged_cols]\n",
    "            check_or_save(merged_df, join(FEATURES,f'dvm_features_{split}_noOH{v}{k}_physical_jittered_50.csv'), index=False, header=False)\n",
    "            check_or_save(physical_only_df, join(FEATURES,f'dvm_features_{split}_noOH{v}{k}_physical_only_jittered_50.csv'), index=False, header=False)\n",
    "        lengths = torch.load(join(FEATURES,f'tabular_lengths{v}.pt'))\n",
    "        new_lengths = [1,1,1,1]\n",
    "        lengths = new_lengths + lengths\n",
    "        check_or_save(lengths, join(FEATURES,f'tabular_lengths{v}_physical.pt'))\n",
    "        lengths = [1,1,1,1,13]\n",
    "        check_or_save(lengths, join(FEATURES,f'tabular_lengths{v}_physical_only.pt'))\n",
    "    \n",
    "physical_df = pd.read_csv(join(TABLES,'Ad_table_physical_filled_jittered_50.csv'))[['Adv_ID', 'Wheelbase','Height','Width','Length']]\n",
    "# TODO k=['', '_0.1', '_0.01'], change k to generate training datasets with different amounts of low data\n",
    "create_feature_files(k='', splits=['train'])\n",
    "create_feature_files(k='_0.9', splits=['train'])\n",
    "create_feature_files(k='_0.99', splits=['train'])\n",
    "create_feature_files(k='', splits=['val', 'test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder features to categorical, numerical\n",
    "for v in ['_all_views']:\n",
    "    field_lengths_tabular = torch.load(join(FEATURES, f'tabular_lengths{v}_physical.pt'))\n",
    "    categorical_ids = []\n",
    "    continous_ids = []\n",
    "    for i in range(len(field_lengths_tabular)):\n",
    "        if field_lengths_tabular[i] == 1:\n",
    "            continous_ids.append(i)\n",
    "        else:\n",
    "            categorical_ids.append(i)\n",
    "    print('Categorical Index: {}, '.format(len(categorical_ids)), categorical_ids)\n",
    "    print('Numerical Index: {}, '.format(len(continous_ids)), continous_ids)\n",
    "\n",
    "    reorder_ids = categorical_ids + continous_ids\n",
    "    reorder_field_lengths_tabular = [field_lengths_tabular[i] for i in reorder_ids]\n",
    "    check_or_save(reorder_field_lengths_tabular, join(FEATURES, f'tabular_lengths{v}_physical_reordered.pt'),)\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        data_tabular = pd.read_csv(join(FEATURES, f'dvm_features_{split}_noOH{v}_physical_jittered_50.csv'), header=None)\n",
    "        reorder_data_tabular = data_tabular.iloc[:, reorder_ids]\n",
    "        check_or_save(reorder_data_tabular, join(FEATURES, f'dvm_features_{split}_noOH{v}_physical_jittered_50_reordered.csv'), index=False, header=False)\n",
    "    for k in ['_0.9', '_0.99']:\n",
    "        data_tabular = pd.read_csv(join(FEATURES, f'dvm_features_train_noOH{v}{k}_physical_jittered_50.csv'), header=None)\n",
    "        reorder_data_tabular = data_tabular.iloc[:, reorder_ids]\n",
    "        check_or_save(reorder_data_tabular, join(FEATURES, f'dvm_features_train_noOH{v}{k}_physical_jittered_50_reordered.csv'), index=False, header=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Labels to Featues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in ['_all_views']:\n",
    "    for split in ['train', 'val']:\n",
    "        labels = torch.load(join(FEATURES,f'labels_model_all_{split}{v}.pt'))\n",
    "        features = pd.read_csv(join(FEATURES,f'dvm_features_{split}_noOH{v}_physical_jittered_50.csv'), header=None)\n",
    "        features['label'] = labels\n",
    "        check_or_save(features, join(FEATURES,f'dvm_features_{split}_noOH{v}_physical_jittered_50_labeled.csv'), index=False, header=False)\n",
    "    lengths = torch.load(join(FEATURES,f'tabular_lengths{v}_physical.pt'))\n",
    "    import builtins\n",
    "    lengths.append(builtins.max(labels)+1)\n",
    "    check_or_save(lengths, join(FEATURES,f'tabular_lengths{v}_physical_labeled.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Adv year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_lengths = torch.load(join(FEATURES, f'tabular_lengths_all_views_physical_reordered.pt'))\n",
    "print(tabular_lengths)\n",
    "tabular_lengths = tabular_lengths[:-1]\n",
    "print(len(tabular_lengths))\n",
    "check_or_save(tabular_lengths, join(FEATURES, f'tabular_lengths_all_views_physical_reordered_rmAY.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reordered_column_name = [ 'Color', 'Bodytype', 'Gearbox','Fuel_type' ,\n",
    "               'Wheelbase', 'Height', 'Width', 'Length', 'Adv_year', 'Adv_month',\n",
    "       'Reg_year', 'Runned_Miles', 'Price', 'Seat_num', 'Door_num',\n",
    "       'Entry_price', 'Engine_size',]\n",
    "column_name = ['Wheelbase', 'Height', 'Width', 'Length', 'Adv_year', 'Adv_month',\n",
    "       'Reg_year', 'Runned_Miles', 'Price', 'Seat_num', 'Door_num',\n",
    "       'Entry_price', 'Engine_size','Color', 'Bodytype', 'Gearbox','Fuel_type']\n",
    "for v in ['', '_0.1', '_0.01']:\n",
    "    for split in ['train']:  # 'train', 'val', 'test'\n",
    "        reordered_features = pd.read_csv(join(FEATURES,f'dvm_features_{split}_noOH_all_views{v}_physical_jittered_50_reordered.csv'), header=None)\n",
    "        reordered_features.columns = reordered_column_name \n",
    "        reordered_features.drop(['Adv_year'], axis=1, inplace=True)\n",
    "        check_or_save(reordered_features, join(FEATURES, f'dvm_features_{split}_noOH_all_views{v}_physical_jittered_50_reordered_rmAY.csv'), index=False, header=False)\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reordered_features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stil",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
